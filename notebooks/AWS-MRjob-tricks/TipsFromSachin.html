<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<title>TipsFromSachin.html</title>

</head>

<body>

<h3>Some other points that keep coming up</h3>

<ul>
<li>To get namenodes and to connect to hdfs the preferred method is to use find<em>waiting</em>flow. You can use the code put up earlier - @37 / follow the cookbook - aws cookbook</li>
<li>You cannot write to a file in hdfs (aws doesnt let us do that), hence write to s3 and pull from there. </li>
<li>Pandas, scikit-learn, numpy etc are now pre installed in the clusters. You can manually install any new package that you might require by sshing into the namenode as explained here - @43. You can manually import files to hdfs after sshing into the namenode as well.</li>
<li>Entire s3 directory can be read as input by giving the s3 directory path as input alongwith the forward slash i.e $input = s3://dse-sachin/inp_directory/ - Thanks Jennifer</li>
<li>Professor has detailed the process of getting logs using get<em>emr</em>logs.py here - <a href="http://seed.ucsd.edu/mediawiki/index.php/AccessingLogsUsingOurScripts">getting logs</a></li>
<li>Its generally helpful to profile your code to optimize execution time. Kevin has explained it in HipChat and you can read more on profilers here - Profilers - Thanks Kevin</li>
</ul>

</body>
</html>
