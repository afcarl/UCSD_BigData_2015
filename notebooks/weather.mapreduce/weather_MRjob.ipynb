{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version:  0.14.0\n",
      "numpy version: 1.8.1\n",
      "sklearn version: 0.14.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "print 'pandas version: ',pd.__version__\n",
    "print 'numpy version:',np.__version__\n",
    "print 'sklearn version:',sk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://yoavfreunddefault/ AKIAIH6ZWS6WZ7FQMZ7A Yoav_Freund\n",
      "total 56056\n",
      "-rw-r--r--  1 yoavfreund  staff    858961 Mar 27 12:23 ALL.corrupted.csv\n",
      "-rw-r--r--  1 yoavfreund  staff    858960 Mar 27 12:23 ALL.head.csv\n",
      "-rw-r--r--  1 yoavfreund  staff    860337 May  3 22:13 F1000.csv\n",
      "drwxr-xr-x  7 yoavfreund  staff       238 Mar 27 12:23 \u001b[34mGHCND-info\u001b[m\u001b[m\n",
      "-rw-r--r--  1 yoavfreund  staff  26114979 Mar 27 12:23 SAMPLE_TMAX.csv\n",
      "/Users/yoavfreund/academic.papers/Courses/BigDataAnalytics/UCSD_BigData/notebooks/weather.mapreduce\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "# Get enviroment variables set from utils/setup.sh\n",
    "home_dir = os.environ['HOME']\n",
    "root_dir = os.environ['BD_GitRoot']\n",
    "\n",
    "# Add utils to the python system path\n",
    "sys.path.append(root_dir + '/utils')\n",
    "# Read AWS credentials from 'EC2_VAULT'/Creds.pkl \n",
    "from read_mrjob_creds import *\n",
    "(key_id, secret_key, s3_bucket, username) = read_credentials()\n",
    "print s3_bucket,key_id,username\n",
    "\n",
    "examples_dir = root_dir + '/data/weather/'\n",
    "!ls -l $examples_dir\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gunzip: can't stat: stations.pkl.gz (stations.pkl.gz.gz): No such file or directory\r\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'stations.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6d85af76c312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'gunzip stations.pkl.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mstations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stations.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'gzip stations.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'stations.pkl'"
     ]
    }
   ],
   "source": [
    "#!/bin/env python\n",
    "import os,sys,re,pickle,coding\n",
    "from numpy import *\n",
    "\n",
    "!gunzip stations.pkl.gz\n",
    "stations=pickle.load(open('stations.pkl', 'rb'))\n",
    "!gzip stations.pkl\n",
    "\n",
    "#Days=pickle.load(open('Dates.pkl', 'rb'))\n",
    "\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASN00054128,DAPR,1969,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\r\n",
      "ASN00054128,DAPR,1971,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3,,,,,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\r\n"
     ]
    }
   ],
   "source": [
    "!head -2 $examples_dir/ALL.head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr_weather.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mr_weather.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "count the number of measurements of each type\n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "from sys import stderr\n",
    "\n",
    "#logfile=open('log','w')\n",
    "logfile=stderr\n",
    "\n",
    "class MRWeather(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        self.increment_counter('MrJob Counters','mapper',1)\n",
    "        elements=line.split(',')\n",
    "        if elements[0]=='station':\n",
    "            yield('header',1)\n",
    "        else:\n",
    "            yield(elements[1],1)\n",
    "            \n",
    "    def combiner(self, word, counts):\n",
    "        self.increment_counter('MrJob Counters','combiner',1)\n",
    "        yield (word, sum(counts))\n",
    "        #l_counts=[c for c in counts]  # extract list from iterator\n",
    "        #S=sum(l_counts)\n",
    "        #logfile.write('combiner '+word+' ['+','.join([str(c) for c in l_counts])+']='+str(S)+'\\n')\n",
    "        #yield (word, S)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        self.increment_counter('MrJob Counters','reducer',1)\n",
    "        yield (word, sum(counts))\n",
    "        #l_counts=[c for c in counts]  # extract list from iterator\n",
    "        #S=sum(l_counts)\n",
    "        #logfile.write('reducer '+word+' ['+','.join([str(c) for c in l_counts])+']='+str(S)+'\\n')\n",
    "        #yield (word, S)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWeather.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/yoavfreund/.mrjob.conf\n",
      "creating tmp directory /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/mr_weather.yoavfreund.20150516.043343.698421\n",
      "writing to /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/mr_weather.yoavfreund.20150516.043343.698421/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  MrJob Counters:\n",
      "    combiner: 21\n",
      "    mapper: 999\n",
      "writing to /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/mr_weather.yoavfreund.20150516.043343.698421/step-0-mapper-sorted\n",
      "> sort /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/mr_weather.yoavfreund.20150516.043343.698421/step-0-mapper_part-00000\n",
      "writing to /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/mr_weather.yoavfreund.20150516.043343.698421/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  MrJob Counters:\n",
      "    combiner: 21\n",
      "    mapper: 999\n",
      "    reducer: 21\n",
      "Moving /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/mr_weather.yoavfreund.20150516.043343.698421/step-0-reducer_part-00000 -> /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/mr_weather.yoavfreund.20150516.043343.698421/output/part-00000\n",
      "Streaming final output from /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/mr_weather.yoavfreund.20150516.043343.698421/output\n",
      "removing tmp directory /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/mr_weather.yoavfreund.20150516.043343.698421\n"
     ]
    }
   ],
   "source": [
    "!python mr_weather.py /Users/yoavfreund/BigData/UCSD_BigData/data/weather/ALL.head.csv > counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DAPR\"\t29\r\n",
      "\"DASF\"\t3\r\n",
      "\"DWPR\"\t17\r\n",
      "\"MDPR\"\t37\r\n",
      "\"MDSF\"\t5\r\n",
      "\"PRCP\"\t420\r\n",
      "\"SNOW\"\t83\r\n",
      "\"SNWD\"\t85\r\n",
      "\"TMAX\"\t123\r\n",
      "\"TMIN\"\t106\r\n",
      "\"TOBS\"\t45\r\n",
      "\"WT01\"\t12\r\n",
      "\"WT03\"\t7\r\n",
      "\"WT04\"\t5\r\n",
      "\"WT05\"\t2\r\n",
      "\"WT06\"\t3\r\n",
      "\"WT08\"\t2\r\n",
      "\"WT11\"\t3\r\n",
      "\"WT14\"\t4\r\n",
      "\"WT16\"\t5\r\n",
      "\"WT18\"\t3\r\n"
     ]
    }
   ],
   "source": [
    "!cat counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL.corrupted.csv ALL.head.csv      F1000.csv         \u001b[34mGHCND-info\u001b[m\u001b[m        SAMPLE_TMAX.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls $examples_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j-153AYJ9PNJYUE WAITING\n",
      "j-2NAO98NKRH37G WAITING\n",
      "j-2UD4PDR1WMGH0 WAITING\n",
      "j-29TWCEID490R1 WAITING\n",
      "j-1K4OUFYCVV74C WAITING\n",
      "j-3V3JJACKWH5O0 WAITING\n",
      "flow_id= j-3V3JJACKWH5O0\n",
      "input_file= hdfs://ec2-54-224-209-44.compute-1.amazonaws.com:9000/weather.raw_data/ALL.csv\n"
     ]
    }
   ],
   "source": [
    "from find_waiting_flow import *\n",
    "flow_id,job_flows = find_waiting_flow(key_id,secret_key)\n",
    "print 'flow_id=',flow_id\n",
    "\n",
    "#find the dns name corresponding to the flow\n",
    "for job in job_flows:\n",
    "    if job.jobflowid == flow_id:\n",
    "        node = job.masterpublicdnsname\n",
    "\n",
    "input_file = 'hdfs://'+node+':9000/weather.raw_data/ALL.csv'\n",
    "print 'input_file=',input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/yoavfreund/.mrjob.conf\n",
      "creating tmp directory /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/mr_weather.yoavfreund.20150516.044217.074280\n",
      "Copying non-input files into s3://yoavfreunddefault/tmp/mr_weather.yoavfreund.20150516.044217.074280/files/\n",
      "Adding our job to existing job flow j-3V3JJACKWH5O0\n",
      "Job launched 31.6s ago, status RUNNING: Running step (mr_weather.yoavfreund.20150516.044217.074280: Step 1 of 1)\n",
      "Job launched 63.2s ago, status RUNNING: Running step (mr_weather.yoavfreund.20150516.044217.074280: Step 1 of 1)\n",
      "Job launched 95.0s ago, status RUNNING: Running step (mr_weather.yoavfreund.20150516.044217.074280: Step 1 of 1)\n",
      "Job launched 126.9s ago, status RUNNING: Running step (mr_weather.yoavfreund.20150516.044217.074280: Step 1 of 1)\n",
      "Job launched 158.6s ago, status RUNNING: Running step (mr_weather.yoavfreund.20150516.044217.074280: Step 1 of 1)\n",
      "Job launched 190.8s ago, status RUNNING: Running step (mr_weather.yoavfreund.20150516.044217.074280: Step 1 of 1)\n",
      "Job launched 222.4s ago, status RUNNING: Running step (mr_weather.yoavfreund.20150516.044217.074280: Step 1 of 1)\n",
      "Job completed.\n",
      "Running time was 234.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters may not have been uploaded to S3 yet. Try again in 5 minutes with: mrjob fetch-logs --counters j-3V3JJACKWH5O0\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Streaming final output from s3://yoavfreunddefault/tmp/mr_weather.yoavfreund.20150516.044217.074280/output/\n",
      "removing tmp directory /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/mr_weather.yoavfreund.20150516.044217.074280\n",
      "Removing all files in s3://yoavfreunddefault/tmp/mr_weather.yoavfreund.20150516.044217.074280/\n"
     ]
    }
   ],
   "source": [
    "#!python mr_weather.py -r emr  --emr-job-flow-id=$flow_id $examples_dir/F1000.csv > counts\n",
    "!python mr_weather.py -r emr  --emr-job-flow-id=$flow_id $input_file > counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ACSC\"\t14\r\n",
      "\"MDEV\"\t10851\r\n",
      "\"MDWM\"\t8429\r\n",
      "\"SN01\"\t571\r\n",
      "\"SN22\"\t29\r\n",
      "\"SNOW\"\t881399\r\n",
      "\"SX03\"\t752\r\n",
      "\"SX51\"\t108\r\n",
      "\"SX72\"\t9\r\n",
      "\"WT09\"\t28469\r\n",
      "\"WT15\"\t4392\r\n",
      "\"WT21\"\t2382\r\n",
      "\"FRGB\"\t113\r\n",
      "\"FRGT\"\t282\r\n",
      "\"MDTN\"\t6162\r\n",
      "\"MNPN\"\t13664\r\n",
      "\"SN14\"\t2\r\n",
      "\"SN35\"\t18\r\n",
      "\"SN56\"\t2\r\n",
      "\"SN83\"\t9\r\n",
      "\"SX01\"\t570\r\n",
      "\"SX22\"\t32\r\n",
      "\"WSFI\"\t77\r\n",
      "\"WT07\"\t19440\r\n",
      "\"WT13\"\t7208\r\n",
      "\"ACSH\"\t9172\r\n",
      "\"DATX\"\t6001\r\n",
      "\"MXPN\"\t13698\r\n",
      "\"SN12\"\t416\r\n",
      "\"SN33\"\t371\r\n",
      "\"SN54\"\t20\r\n",
      "\"SN81\"\t9\r\n",
      "\"SX14\"\t3\r\n",
      "\"SX35\"\t22\r\n",
      "\"SX56\"\t2\r\n",
      "\"SX83\"\t9\r\n",
      "\"THIC\"\t452\r\n",
      "\"WDF2\"\t13658\r\n",
      "\"WDFM\"\t3201\r\n",
      "\"WSF5\"\t13482\r\n",
      "\"WSFG\"\t14486\r\n",
      "\"WT05\"\t148289\r\n",
      "\"WT11\"\t120277\r\n",
      "\"ACMC\"\t13\r\n",
      "\"PGTM\"\t26220\r\n",
      "\"SN31\"\t357\r\n",
      "\"SN52\"\t936\r\n",
      "\"SX12\"\t416\r\n",
      "\"SX33\"\t372\r\n",
      "\"SX54\"\t20\r\n",
      "\"SX81\"\t9\r\n",
      "\"WESF\"\t31503\r\n",
      "\"WT03\"\t272406\r\n",
      "\"WT18\"\t53072\r\n",
      "\"WV07\"\t63\r\n",
      "\"header\"\t1\r\n",
      "\"DAPR\"\t299139\r\n",
      "\"DASF\"\t17195\r\n",
      "\"GAHT\"\t936\r\n",
      "\"SN02\"\t3374\r\n",
      "\"SN23\"\t27\r\n",
      "\"SX31\"\t357\r\n",
      "\"SX52\"\t941\r\n",
      "\"WDFI\"\t72\r\n",
      "\"WESD\"\t55740\r\n",
      "\"WSF1\"\t5344\r\n",
      "\"WT01\"\t239625\r\n",
      "\"WT16\"\t73840\r\n",
      "\"WT22\"\t5469\r\n",
      "\"ACMH\"\t7960\r\n",
      "\"FMTM\"\t13372\r\n",
      "\"MDTX\"\t6001\r\n",
      "\"SN21\"\t28\r\n",
      "\"SN36\"\t5\r\n",
      "\"SX02\"\t3373\r\n",
      "\"SX17\"\t2\r\n",
      "\"SX23\"\t27\r\n",
      "\"WDF5\"\t13480\r\n",
      "\"WDFG\"\t13759\r\n",
      "\"WT08\"\t53648\r\n",
      "\"WT14\"\t38369\r\n",
      "\"WV03\"\t1540\r\n",
      "\"WV18\"\t15\r\n",
      "\"DAEV\"\t10849\r\n",
      "\"DAWM\"\t8428\r\n",
      "\"EVAP\"\t24827\r\n",
      "\"PRCP\"\t2521007\r\n",
      "\"SN13\"\t60\r\n",
      "\"SN34\"\t11\r\n",
      "\"SN55\"\t20\r\n",
      "\"SN61\"\t22\r\n",
      "\"SN82\"\t9\r\n",
      "\"SX15\"\t2\r\n",
      "\"SX21\"\t29\r\n",
      "\"SX36\"\t7\r\n",
      "\"TMIN\"\t969579\r\n",
      "\"TOBS\"\t478981\r\n",
      "\"TSUN\"\t10557\r\n",
      "\"WDMV\"\t24052\r\n",
      "\"WT06\"\t103479\r\n",
      "\"WT12\"\t84\r\n",
      "\"WV01\"\t397\r\n",
      "\"AWND\"\t17950\r\n",
      "\"DATN\"\t6162\r\n",
      "\"MDPR\"\t349252\r\n",
      "\"MDSF\"\t20365\r\n",
      "\"SN11\"\t38\r\n",
      "\"SN32\"\t2405\r\n",
      "\"SN53\"\t125\r\n",
      "\"SNWD\"\t864192\r\n",
      "\"SX13\"\t60\r\n",
      "\"SX34\"\t12\r\n",
      "\"SX55\"\t23\r\n",
      "\"SX61\"\t23\r\n",
      "\"SX82\"\t9\r\n",
      "\"WDF1\"\t5319\r\n",
      "\"WT04\"\t163690\r\n",
      "\"WT10\"\t2268\r\n",
      "\"WT19\"\t4631\r\n",
      "\"WV20\"\t604\r\n",
      "\"DWPR\"\t180462\r\n",
      "\"FRTH\"\t135\r\n",
      "\"PSUN\"\t3322\r\n",
      "\"SN03\"\t750\r\n",
      "\"SN51\"\t108\r\n",
      "\"SN72\"\t9\r\n",
      "\"SX11\"\t38\r\n",
      "\"SX32\"\t2405\r\n",
      "\"SX53\"\t124\r\n",
      "\"TMAX\"\t967931\r\n",
      "\"WSF2\"\t13657\r\n",
      "\"WSFM\"\t3204\r\n",
      "\"WT02\"\t22199\r\n",
      "\"WT17\"\t6945\r\n"
     ]
    }
   ],
   "source": [
    "!cat counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two useful command-line utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the mrjob command line depends on the same configuration file as the run-time library. This configuration file is, by default, located at ~/.mrjob.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: mrjob {subcommand|--help}\"\r\n",
      "\r\n",
      "subcommands:\r\n",
      "  audit-emr-usage:          Audit EMR usage\r\n",
      "  create-job-flow:          Create an EMR job flow\r\n",
      "  fetch-logs:               Fetch and parse EMR logs for errors and counters\r\n",
      "  report-long-jobs:         Report EMR jobs which have been running for a long time\r\n",
      "  run:                      Run a job\r\n",
      "  s3-tmpwatch:              Delete S3 keys older than a specified time\r\n",
      "  terminate-idle-job-flows: Terminate idle EMR job flows\r\n",
      "  terminate-job-flow:       Terminate a single EMR job flow\r\n"
     ]
    }
   ],
   "source": [
    "!mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting job flow history...\n",
      "using configs in /home/ubuntu/.mrjob.conf\n",
      "compiling job flow stats...\n",
      "Total  # of Job Flows: 17\n",
      "\n",
      "* All times are in UTC.\n",
      "\n",
      "Min create time: 2014-05-04 05:33:15\n",
      "Max create time: 2014-05-09 13:19:58\n",
      "   Current time: 2014-05-10 01:01:31\n",
      "\n",
      "* All usage is measured in Normalized Instance Hours, which are\n",
      "  roughly equivalent to running an m1.small instance for an hour.\n",
      "  Billing is estimated, and may not match Amazon's system exactly.\n",
      "\n",
      "Total billed:    1662.08  100.0%\n",
      "  Total used:      25.65    1.5%\n",
      "    bootstrap:      5.53    0.3%\n",
      "    jobs:          20.12    1.2%\n",
      "  Total waste:   1636.43   98.5%\n",
      "    at end:       494.39   29.7%\n",
      "    other:       1142.04   68.7%\n",
      "\n",
      "Daily statistics:\n",
      "\n",
      " date          billed      used     waste   % waste\n",
      " 2014-05-10     31.70      3.36     28.34      89.4\n",
      " 2014-05-09    397.09     15.47    381.62      96.1\n",
      " 2014-05-08    573.37      1.93    571.45      99.7\n",
      " 2014-05-07    314.41      1.30    313.11      99.6\n",
      " 2014-05-06     48.50      0.66     47.85      98.6\n",
      " 2014-05-05    206.35      0.00    206.35     100.0\n",
      " 2014-05-04     90.65      2.95     87.70      96.8\n",
      "\n",
      "Hourly statistics:\n",
      "\n",
      " hour              billed      used     waste   % waste\n",
      " 2014-05-10 01       0.78      0.00      0.78     100.0\n",
      " 2014-05-10 00      30.92      3.36     27.56      89.1\n",
      " 2014-05-09 23      30.92      8.73     22.19      71.8\n",
      " 2014-05-09 22      30.92      0.88     30.04      97.2\n",
      " 2014-05-09 21      30.92      0.00     30.92     100.0\n",
      " 2014-05-09 20      30.92      0.00     30.92     100.0\n",
      " 2014-05-09 19      30.92      1.10     29.82      96.4\n",
      " 2014-05-09 18      30.92      0.00     30.92     100.0\n",
      " 2014-05-09 17      30.92      1.15     29.77      96.3\n",
      " 2014-05-09 16      30.92      1.19     29.72      96.1\n",
      " 2014-05-09 15      30.92      0.00     30.92     100.0\n",
      " 2014-05-09 14      30.92      0.62     30.30      98.0\n",
      " 2014-05-09 13      17.21      0.38     16.83      97.8\n",
      " 2014-05-09 12       0.00      0.00      0.00       0.0\n",
      " 2014-05-09 11       0.00      0.00      0.00       0.0\n",
      " 2014-05-09 10       0.00      0.00      0.00       0.0\n",
      " 2014-05-09 09       0.00      0.00      0.00       0.0\n",
      " 2014-05-09 08       0.00      0.00      0.00       0.0\n",
      " 2014-05-09 07       0.00      0.00      0.00       0.0\n",
      " 2014-05-09 06       0.00      0.00      0.00       0.0\n",
      " 2014-05-09 05       0.00      0.00      0.00       0.0\n",
      " 2014-05-09 04       0.00      0.00      0.00       0.0\n",
      " 2014-05-09 03       0.00      0.00      0.00       0.0\n",
      " 2014-05-09 02      13.71      0.00     13.71     100.0\n",
      " 2014-05-09 01      28.67      0.00     28.67     100.0\n",
      " 2014-05-09 00      28.33      1.43     26.91      95.0\n",
      " 2014-05-08 23      28.00      0.00     28.00     100.0\n",
      " 2014-05-08 22      28.00      0.00     28.00     100.0\n",
      " 2014-05-08 21      28.00      0.00     28.00     100.0\n",
      " 2014-05-08 20      28.00      0.00     28.00     100.0\n",
      " 2014-05-08 19      28.00      0.00     28.00     100.0\n",
      " 2014-05-08 18      28.00      0.00     28.00     100.0\n",
      " 2014-05-08 17      28.00      0.00     28.00     100.0\n",
      " 2014-05-08 16      28.00      0.00     28.00     100.0\n",
      " 2014-05-08 15      28.28      0.00     28.28     100.0\n",
      " 2014-05-08 14      28.72      0.07     28.65      99.8\n",
      " 2014-05-08 13      28.00      0.00     28.00     100.0\n",
      " 2014-05-08 12      28.00      0.00     28.00     100.0\n",
      " 2014-05-08 11      28.00      0.00     28.00     100.0\n",
      " 2014-05-08 10      28.00      0.00     28.00     100.0\n",
      " 2014-05-08 09      28.00      0.00     28.00     100.0\n",
      " 2014-05-08 08      28.00      0.00     28.00     100.0\n",
      " 2014-05-08 07      28.00      0.00     28.00     100.0\n",
      " 2014-05-08 06      18.02      1.86     16.16      89.7\n",
      " 2014-05-08 05      13.06      0.00     13.06     100.0\n",
      " 2014-05-08 04      13.06      0.00     13.06     100.0\n",
      " 2014-05-08 03      13.06      0.00     13.06     100.0\n",
      " 2014-05-08 02      13.06      0.00     13.06     100.0\n",
      " 2014-05-08 01      13.06      0.00     13.06     100.0\n",
      " 2014-05-08 00      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 23      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 22      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 21      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 20      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 19      13.06      0.31     12.75      97.6\n",
      " 2014-05-07 18      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 17      13.41      0.31     13.10      97.7\n",
      " 2014-05-07 16      13.71      0.68     13.03      95.1\n",
      " 2014-05-07 15      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 14      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 13      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 12      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 11      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 10      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 09      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 08      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 07      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 06      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 05      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 04      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 03      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 02      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 01      13.06      0.00     13.06     100.0\n",
      " 2014-05-07 00      13.06      0.00     13.06     100.0\n",
      " 2014-05-06 23      13.06      0.00     13.06     100.0\n",
      " 2014-05-06 22      13.06      0.00     13.06     100.0\n",
      " 2014-05-06 21      13.06      0.00     13.06     100.0\n",
      " 2014-05-06 20       9.33      0.66      8.67      93.0\n",
      " 2014-05-06 19       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 18       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 17       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 16       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 15       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 14       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 13       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 12       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 11       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 10       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 09       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 08       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 07       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 06       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 05       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 04       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 03       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 02       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 01       0.00      0.00      0.00       0.0\n",
      " 2014-05-06 00       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 23       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 22       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 21       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 20       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 19       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 18       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 17       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 16       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 15       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 14       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 13       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 12       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 11       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 10       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 09       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 08       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 07       0.00      0.00      0.00       0.0\n",
      " 2014-05-05 06       8.35      0.00      8.35     100.0\n",
      " 2014-05-05 05      33.00      0.00     33.00     100.0\n",
      " 2014-05-05 04      33.00      0.00     33.00     100.0\n",
      " 2014-05-05 03      33.00      0.00     33.00     100.0\n",
      " 2014-05-05 02      33.00      0.00     33.00     100.0\n",
      " 2014-05-05 01      33.00      0.00     33.00     100.0\n",
      " 2014-05-05 00      33.00      0.00     33.00     100.0\n",
      " 2014-05-04 23      27.62      1.09     26.53      96.1\n",
      " 2014-05-04 22      14.00      0.00     14.00     100.0\n",
      " 2014-05-04 21      14.00      0.00     14.00     100.0\n",
      " 2014-05-04 20      11.03      0.69     10.34      93.8\n",
      " 2014-05-04 19       0.00      0.00      0.00       0.0\n",
      " 2014-05-04 18       0.00      0.00      0.00       0.0\n",
      " 2014-05-04 17       0.00      0.00      0.00       0.0\n",
      " 2014-05-04 16       0.00      0.00      0.00       0.0\n",
      " 2014-05-04 15       0.00      0.00      0.00       0.0\n",
      " 2014-05-04 14       0.00      0.00      0.00       0.0\n",
      " 2014-05-04 13       0.00      0.00      0.00       0.0\n",
      " 2014-05-04 12       0.00      0.00      0.00       0.0\n",
      " 2014-05-04 11       0.00      0.00      0.00       0.0\n",
      " 2014-05-04 10       0.00      0.00      0.00       0.0\n",
      " 2014-05-04 09       0.00      0.00      0.00       0.0\n",
      " 2014-05-04 08       0.00      0.00      0.00       0.0\n",
      " 2014-05-04 07       0.00      0.00      0.00       0.0\n",
      " 2014-05-04 06      14.38      0.00     14.38     100.0\n",
      " 2014-05-04 05       9.62      1.17      8.45      87.9\n",
      "\n",
      "* Job flows are considered to belong to the user and job that\n",
      "  started them or last ran on them.\n",
      "\n",
      "Top jobs, by total time used:\n",
      "      15.84 mr_weather\n",
      "       5.46 None\n",
      "       3.97 mr_word_freq_count\n",
      "       0.38 no_script\n",
      "\n",
      "Top jobs, by time billed but not used:\n",
      "    1051.88 None\n",
      "     286.85 mr_word_freq_count\n",
      "     253.22 mr_weather\n",
      "      44.49 no_script\n",
      "\n",
      "Top users, by total time used:\n",
      "      19.11 yoavfreund\n",
      "       5.46 None\n",
      "       1.08 ubuntu\n",
      "\n",
      "Top users, by time billed but not used:\n",
      "    1051.88 None\n",
      "     577.06 yoavfreund\n",
      "       7.49 ubuntu\n",
      "\n",
      "Top job steps, by total time used (step number first):\n",
      "      15.84   1 mr_weather\n",
      "       3.88   1 mr_word_freq_count\n",
      "       0.40     (non-mrjob step)\n",
      "\n",
      "Top job steps, by total time billed but not used (un-pooled only):\n",
      "    1051.92     (non-mrjob step)\n",
      "     286.85   1 mr_word_freq_count\n",
      "     253.22   1 mr_weather\n",
      "\n",
      "All pools, by total time billed:\n",
      "    1662.08 (not pooled)\n",
      "\n",
      "All pools, by total time billed but not used:\n",
      "    1636.43 (not pooled)\n",
      "\n",
      "All job flows, by total time billed:\n",
      "     560.00 j-35O01QLMRUFED My cluster\n",
      "     444.00 j-3MMGSXIO3FQR3 My cluster\n",
      "     358.08 j-1HFD8T7K9LGVH no_script.yoavfreund.20140509.131949.007708\n",
      "     231.00 j-19ZLC6D8RI1E4 My Bigger Cluster\n",
      "      42.00 j-KZSDWFOETODE  My Bigger Cluster\n",
      "      24.00 j-2DP66KPYB7NHE My Bigger Cluster\n",
      "       1.00 j-NXJGTL9EFM35  mr_word_freq_count.yoavfreund.20140507.161040.869606\n",
      "       1.00 j-1QCXB5Q0VHRB2 mr_word_freq_count.yoavfreund.20140508.141408.998481\n",
      "       1.00 j-3IR7VIA9RA2TH mr_word_freq_count.yoavfreund.20140509.003734.519811\n",
      "       0.00 j-A0JHEG86PAGJ  A 6 node spot market cluster\n",
      "       0.00 j-2M58Y9QSWOIFH A 6 node spot market cluster\n",
      "       0.00 j-13O1ZEWJ0595Q A 6 node spot market cluster\n",
      "       0.00 j-1BCZIG7KD2S0K My Bigger Cluster\n",
      "       0.00 j-6ZU44Z9AXTQ3  My Bigger Cluster\n",
      "       0.00 j-2E4BUM0ADD3MX My Bigger Cluster\n",
      "       0.00 j-3T3SHY8GBF6H4 no_script.yoavfreund.20140509.065034.068893\n",
      "       0.00 j-33FAYHWEI1FP  no_script.yoavfreund.20140509.065544.819488\n",
      "\n",
      "All job flows, by time billed but not used:\n",
      "     556.78 j-35O01QLMRUFED My cluster\n",
      "     442.11 j-3MMGSXIO3FQR3 My cluster\n",
      "     340.68 j-1HFD8T7K9LGVH no_script.yoavfreund.20140509.131949.007708\n",
      "     229.91 j-19ZLC6D8RI1E4 My Bigger Cluster\n",
      "      41.31 j-KZSDWFOETODE  My Bigger Cluster\n",
      "      22.83 j-2DP66KPYB7NHE My Bigger Cluster\n",
      "       0.94 j-3IR7VIA9RA2TH mr_word_freq_count.yoavfreund.20140509.003734.519811\n",
      "       0.94 j-NXJGTL9EFM35  mr_word_freq_count.yoavfreund.20140507.161040.869606\n",
      "       0.93 j-1QCXB5Q0VHRB2 mr_word_freq_count.yoavfreund.20140508.141408.998481\n",
      "       0.00 j-A0JHEG86PAGJ  A 6 node spot market cluster\n",
      "       0.00 j-2M58Y9QSWOIFH A 6 node spot market cluster\n",
      "       0.00 j-13O1ZEWJ0595Q A 6 node spot market cluster\n",
      "       0.00 j-1BCZIG7KD2S0K My Bigger Cluster\n",
      "       0.00 j-6ZU44Z9AXTQ3  My Bigger Cluster\n",
      "       0.00 j-2E4BUM0ADD3MX My Bigger Cluster\n",
      "       0.00 j-3T3SHY8GBF6H4 no_script.yoavfreund.20140509.065034.068893\n",
      "       0.00 j-33FAYHWEI1FP  no_script.yoavfreund.20140509.065544.819488\n",
      "\n",
      "Details for all job flows:\n",
      "\n",
      " id              state         created             steps        time ran     billed    waste   user   name\n",
      " j-1HFD8T7K9LGVH WAITING       2014-05-09 13:19:58  17          11:34:55     17.40    340.68 yoavfreund no_script\n",
      " j-33FAYHWEI1FP  TERMINATED    2014-05-09 06:55:53   0           0:00:00      0.00      0.00 yoavfreund no_script\n",
      " j-3T3SHY8GBF6H4 TERMINATED    2014-05-09 06:50:43   0           0:00:00      0.00      0.00 yoavfreund no_script\n",
      " j-3IR7VIA9RA2TH COMPLETED     2014-05-09 00:37:44   1           0:04:53      0.06      0.94 yoavfreund mr_word_freq_count\n",
      " j-1QCXB5Q0VHRB2 COMPLETED     2014-05-08 14:14:19   1           0:05:08      0.07      0.93 yoavfreund mr_word_freq_count\n",
      " j-35O01QLMRUFED TERMINATED    2014-05-08 06:19:21   3          19:34:35      3.22    556.78          not started by mrjob\n",
      " j-NXJGTL9EFM35  COMPLETED     2014-05-07 16:10:51   1           0:05:00      0.06      0.94 yoavfreund mr_word_freq_count\n",
      " j-A0JHEG86PAGJ  FAILED        2014-05-07 00:43:12   1           0:00:00      0.00      0.00          not started by mrjob\n",
      " j-3MMGSXIO3FQR3 TERMINATED    2014-05-06 20:14:11   5    1 day, 9:55:15      1.89    442.11          not started by mrjob\n",
      " j-2M58Y9QSWOIFH FAILED        2014-05-06 15:30:34   1           0:00:00      0.00      0.00          not started by mrjob\n",
      " j-13O1ZEWJ0595Q FAILED        2014-05-06 04:52:07   1           0:00:00      0.00      0.00          not started by mrjob\n",
      " j-1BCZIG7KD2S0K FAILED        2014-05-06 03:15:25   1           0:00:00      0.00      0.00          not started by mrjob\n",
      " j-6ZU44Z9AXTQ3  FAILED        2014-05-06 00:29:50   1           0:00:00      0.00      0.00          not started by mrjob\n",
      " j-2E4BUM0ADD3MX FAILED        2014-05-06 00:10:53   1           0:00:00      0.00      0.00          not started by mrjob\n",
      " j-19ZLC6D8RI1E4 TERMINATED    2014-05-04 23:12:39   1           6:20:55      1.09    229.91          not started by mrjob\n",
      " j-KZSDWFOETODE  TERMINATED    2014-05-04 20:09:33   1           2:45:09      0.69     41.31          not started by mrjob\n",
      " j-2DP66KPYB7NHE TERMINATED    2014-05-04 05:33:15   1           0:19:44      1.17     22.83          not started by mrjob\n"
     ]
    }
   ],
   "source": [
    "!mrjob audit-emr-usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s3cmd is a utility that makes it easy to work with s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: s3cmd [options] COMMAND [parameters]\r\n",
      "\r\n",
      "S3cmd is a tool for managing objects in Amazon S3 storage. It allows for\r\n",
      "making and removing \"buckets\" and uploading, downloading and removing\r\n",
      "\"objects\" from these buckets.\r\n",
      "\r\n",
      "Options:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --configure           Invoke interactive (re)configuration tool.\r\n",
      "  -c FILE, --config=FILE\r\n",
      "                        Config file name. Defaults to /home/ubuntu/.s3cfg\r\n",
      "  --dump-config         Dump current configuration after parsing config files\r\n",
      "                        and command line options and exit.\r\n",
      "  -n, --dry-run         Only show what should be uploaded or downloaded but\r\n",
      "                        don't actually do it. May still perform S3 requests to\r\n",
      "                        get bucket listings and other information though (only\r\n",
      "                        for file transfer commands)\r\n",
      "  -e, --encrypt         Encrypt files before uploading to S3.\r\n",
      "  --no-encrypt          Don't encrypt files.\r\n",
      "  -f, --force           Force overwrite and other dangerous operations.\r\n",
      "  --continue            Continue getting a partially downloaded file (only for\r\n",
      "                        [get] command).\r\n",
      "  --skip-existing       Skip over files that exist at the destination (only\r\n",
      "                        for [get] and [sync] commands).\r\n",
      "  -r, --recursive       Recursive upload, download or removal.\r\n",
      "  --check-md5           Check MD5 sums when comparing files for [sync].\r\n",
      "                        (default)\r\n",
      "  --no-check-md5        Do not check MD5 sums when comparing files for [sync].\r\n",
      "                        Only size will be compared. May significantly speed up\r\n",
      "                        transfer but may also miss some changed files.\r\n",
      "  -P, --acl-public      Store objects with ACL allowing read for anyone.\r\n",
      "  --acl-private         Store objects with default ACL allowing access for you\r\n",
      "                        only.\r\n",
      "  --acl-grant=PERMISSION:EMAIL or USER_CANONICAL_ID\r\n",
      "                        Grant stated permission to a given amazon user.\r\n",
      "                        Permission is one of: read, write, read_acp,\r\n",
      "                        write_acp, full_control, all\r\n",
      "  --acl-revoke=PERMISSION:USER_CANONICAL_ID\r\n",
      "                        Revoke stated permission for a given amazon user.\r\n",
      "                        Permission is one of: read, write, read_acp, wr\r\n",
      "                        ite_acp, full_control, all\r\n",
      "  --delete-removed      Delete remote objects with no corresponding local file\r\n",
      "                        [sync]\r\n",
      "  --no-delete-removed   Don't delete remote objects.\r\n",
      "  -p, --preserve        Preserve filesystem attributes (mode, ownership,\r\n",
      "                        timestamps). Default for [sync] command.\r\n",
      "  --no-preserve         Don't store FS attributes\r\n",
      "  --exclude=GLOB        Filenames and paths matching GLOB will be excluded\r\n",
      "                        from sync\r\n",
      "  --exclude-from=FILE   Read --exclude GLOBs from FILE\r\n",
      "  --rexclude=REGEXP     Filenames and paths matching REGEXP (regular\r\n",
      "                        expression) will be excluded from sync\r\n",
      "  --rexclude-from=FILE  Read --rexclude REGEXPs from FILE\r\n",
      "  --include=GLOB        Filenames and paths matching GLOB will be included\r\n",
      "                        even if previously excluded by one of\r\n",
      "                        --(r)exclude(-from) patterns\r\n",
      "  --include-from=FILE   Read --include GLOBs from FILE\r\n",
      "  --rinclude=REGEXP     Same as --include but uses REGEXP (regular expression)\r\n",
      "                        instead of GLOB\r\n",
      "  --rinclude-from=FILE  Read --rinclude REGEXPs from FILE\r\n",
      "  --bucket-location=BUCKET_LOCATION\r\n",
      "                        Datacentre to create bucket in. As of now the\r\n",
      "                        datacenters are: US (default), EU, us-west-1, and ap-\r\n",
      "                        southeast-1\r\n",
      "  --reduced-redundancy, --rr\r\n",
      "                        Store object with 'Reduced redundancy'. Lower per-GB\r\n",
      "                        price. [put, cp, mv]\r\n",
      "  --access-logging-target-prefix=LOG_TARGET_PREFIX\r\n",
      "                        Target prefix for access logs (S3 URI) (for [cfmodify]\r\n",
      "                        and [accesslog] commands)\r\n",
      "  --no-access-logging   Disable access logging (for [cfmodify] and [accesslog]\r\n",
      "                        commands)\r\n",
      "  -m MIME/TYPE, --mime-type=MIME/TYPE\r\n",
      "                        Default MIME-type to be set for objects stored.\r\n",
      "  -M, --guess-mime-type\r\n",
      "                        Guess MIME-type of files by their extension. Falls\r\n",
      "                        back to default MIME-Type as specified by --mime-type\r\n",
      "                        option\r\n",
      "  --add-header=NAME:VALUE\r\n",
      "                        Add a given HTTP header to the upload request. Can be\r\n",
      "                        used multiple times. For instance set 'Expires' or\r\n",
      "                        'Cache-Control' headers (or both) using this options\r\n",
      "                        if you like.\r\n",
      "  --encoding=ENCODING   Override autodetected terminal and filesystem encoding\r\n",
      "                        (character set). Autodetected: UTF-8\r\n",
      "  --verbatim            Use the S3 name as given on the command line. No pre-\r\n",
      "                        processing, encoding, etc. Use with caution!\r\n",
      "  --list-md5            Include MD5 sums in bucket listings (only for 'ls'\r\n",
      "                        command).\r\n",
      "  -H, --human-readable-sizes\r\n",
      "                        Print sizes in human readable form (eg 1kB instead of\r\n",
      "                        1234).\r\n",
      "  --progress            Display progress meter (default on TTY).\r\n",
      "  --no-progress         Don't display progress meter (default on non-TTY).\r\n",
      "  --enable              Enable given CloudFront distribution (only for\r\n",
      "                        [cfmodify] command)\r\n",
      "  --disable             Enable given CloudFront distribution (only for\r\n",
      "                        [cfmodify] command)\r\n",
      "  --cf-add-cname=CNAME  Add given CNAME to a CloudFront distribution (only for\r\n",
      "                        [cfcreate] and [cfmodify] commands)\r\n",
      "  --cf-remove-cname=CNAME\r\n",
      "                        Remove given CNAME from a CloudFront distribution\r\n",
      "                        (only for [cfmodify] command)\r\n",
      "  --cf-comment=COMMENT  Set COMMENT for a given CloudFront distribution (only\r\n",
      "                        for [cfcreate] and [cfmodify] commands)\r\n",
      "  --cf-default-root-object=DEFAULT_ROOT_OBJECT\r\n",
      "                        Set the default root object to return when no object\r\n",
      "                        is specified in the URL. Use a relative path, i.e.\r\n",
      "                        default/index.html instead of /default/index.html or\r\n",
      "                        s3://bucket/default/index.html (only for [cfcreate]\r\n",
      "                        and [cfmodify] commands)\r\n",
      "  -v, --verbose         Enable verbose output.\r\n",
      "  -d, --debug           Enable debug output.\r\n",
      "  --version             Show s3cmd version (1.0.0) and exit.\r\n",
      "  -F, --follow-symlinks\r\n",
      "                        Follow symbolic links as if they are regular files\r\n",
      "\r\n",
      "Commands:\r\n",
      "  Make bucket\r\n",
      "      s3cmd mb s3://BUCKET\r\n",
      "  Remove bucket\r\n",
      "      s3cmd rb s3://BUCKET\r\n",
      "  List objects or buckets\r\n",
      "      s3cmd ls [s3://BUCKET[/PREFIX]]\r\n",
      "  List all object in all buckets\r\n",
      "      s3cmd la \r\n",
      "  Put file into bucket\r\n",
      "      s3cmd put FILE [FILE...] s3://BUCKET[/PREFIX]\r\n",
      "  Get file from bucket\r\n",
      "      s3cmd get s3://BUCKET/OBJECT LOCAL_FILE\r\n",
      "  Delete file from bucket\r\n",
      "      s3cmd del s3://BUCKET/OBJECT\r\n",
      "  Synchronize a directory tree to S3\r\n",
      "      s3cmd sync LOCAL_DIR s3://BUCKET[/PREFIX] or s3://BUCKET[/PREFIX] LOCAL_DIR\r\n",
      "  Disk usage by buckets\r\n",
      "      s3cmd du [s3://BUCKET[/PREFIX]]\r\n",
      "  Get various information about Buckets or Files\r\n",
      "      s3cmd info s3://BUCKET[/OBJECT]\r\n",
      "  Copy object\r\n",
      "      s3cmd cp s3://BUCKET1/OBJECT1 s3://BUCKET2[/OBJECT2]\r\n",
      "  Move object\r\n",
      "      s3cmd mv s3://BUCKET1/OBJECT1 s3://BUCKET2[/OBJECT2]\r\n",
      "  Modify Access control list for Bucket or Files\r\n",
      "      s3cmd setacl s3://BUCKET[/OBJECT]\r\n",
      "  Enable/disable bucket access logging\r\n",
      "      s3cmd accesslog s3://BUCKET\r\n",
      "  Sign arbitrary string using the secret key\r\n",
      "      s3cmd sign STRING-TO-SIGN\r\n",
      "  Fix invalid file names in a bucket\r\n",
      "      s3cmd fixbucket s3://BUCKET[/PREFIX]\r\n",
      "  List CloudFront distribution points\r\n",
      "      s3cmd cflist \r\n",
      "  Display CloudFront distribution point parameters\r\n",
      "      s3cmd cfinfo [cf://DIST_ID]\r\n",
      "  Create CloudFront distribution point\r\n",
      "      s3cmd cfcreate s3://BUCKET\r\n",
      "  Delete CloudFront distribution point\r\n",
      "      s3cmd cfdelete cf://DIST_ID\r\n",
      "  Change CloudFront distribution point parameters\r\n",
      "      s3cmd cfmodify cf://DIST_ID\r\n",
      "\r\n",
      "For more informations see the progect homepage:\r\n",
      "http://s3tools.org\r\n",
      "\r\n",
      "Consider a donation if you have found s3cmd useful:\r\n",
      "http://s3tools.org/donate\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!s3cmd --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load /home/ubuntu/UCSD_BigData/utils/AWS_keypair_management.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/ubuntu/UCSD_BigData/utils/AWS_keypair_management.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/ubuntu/UCSD_BigData/utils/AWS_keypair_management.py\n",
    "from glob import glob\n",
    "from string import strip\n",
    "from os import chdir\n",
    "from mrjob.emr import EMRJobRunner\n",
    "\n",
    "def test_key_pair(Access_Key_Id,Secret_Access_Key):\n",
    "    try:\n",
    "        JobRunner = EMRJobRunner(aws_access_key_id=Access_Key_Id,aws_secret_access_key=Secret_Access_Key)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def Get_Working_Credentials(path):\n",
    "    \"\"\" check all files in the path directory, find the files that \n",
    "    contain key-pairs in the format downloaded from AWS and check which\n",
    "    of these AWS key pairs is active.\n",
    "    \"\"\"\n",
    "    chdir(path)\n",
    "    credentials_header='User Name,Access Key Id,Secret Access Key'\n",
    "    Key_Table={}\n",
    "    bad_key_files=[]\n",
    "    for filename in glob('*'):\n",
    "        with open(filename,'r') as file:\n",
    "            header_line=strip(file.readline())\n",
    "            if header_line==credentials_header:\n",
    "                # print '\"%s\"'%header_line\n",
    "                for line in file.readlines():\n",
    "                    (User_Name,Access_Key_Id,Secret_Access_Key)=strip(line).split(',')\n",
    "                    User_Name=User_Name[1:-1]\n",
    "                    print filename,User_Name,Access_Key_Id,Secret_Access_Key\n",
    "                    if test_key_pair(Access_Key_Id,Secret_Access_Key):\n",
    "                        print \"an active key pair\"\n",
    "                        if not User_Name in Key_Table.keys():\n",
    "                            Key_Table[User_Name]=[]\n",
    "                        Key_Table[User_Name].append({\n",
    "                            'Access_Key_Id':Access_Key_Id,'Secret_Access_Key':Secret_Access_Key})\n",
    "                    else:\n",
    "                        print filename,\"an inactive key pair\"\n",
    "                        bad_key_files.append(filename)\n",
    "    return Key_Table,bad_key_files\n",
    "#Get_Working_Credentials('../../Vault/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Create a persistent EMR job flow to run jobs in. WARNING: do not run this\r\n",
      "# without mrjob.tools.emr.terminate_idle_job_flows in your crontab; job flows\r\n",
      "# left idle can quickly become expensive!\r\n",
      "runners:\r\n",
      "  name: MRJob-Cluster-6\r\n",
      "  owner: Yoav_Freund\r\n",
      "  emr:\r\n",
      "    # Region to connect to S3 and EMR on (e.g. us-west-1).\r\n",
      "    aws_region: us-east-1\r\n",
      "    # Availability zone to run the job flow on\r\n",
      "    aws_availability_zone: us-east-1d\r\n",
      "    aws_access_key_id: AKIAI2I4AYCHMMTGO2VA\r\n",
      "    aws_secret_access_key: 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\r\n",
      "    # alternate tmp dir\r\n",
      "    #base_tmp_dir: /scratch/mrjob\r\n",
      "    #bootstrap:\r\n",
      "    #\t- sudo apt-get install python-pandas\r\n",
      "    #bootstrap_action: BOOTSTRAP_ACTIONS\r\n",
      "    #                    Raw bootstrap action scripts to run before any of the\r\n",
      "    #                    other bootstrap steps. You can use bootstrap-action\r\n",
      "    #                    more than once. Local scripts will be automatically\r\n",
      "    #                    uploaded to S3. To add arguments, just use quotes:\r\n",
      "    #                    \"foo.sh arg1 arg2\"\r\n",
      "    #bootstrap_cmd: BOOTSTRAP_CMDS\r\n",
      "    #                    Commands to run on the master node to set up\r\n",
      "    #                    libraries, etc. You can use bootstrap-cmd more than\r\n",
      "    #                    once. Use mrjob.conf to specify arguments as a list to\r\n",
      "    #                    be run directly.\r\n",
      "    #bootstrap_file: BOOTSTRAP_FILES\r\n",
      "    #                    File to upload to the master node before running\r\n",
      "    #                    bootstrap_cmds (for example, debian packages). These\r\n",
      "    #                    will be made public on S3 due to a limitation of the\r\n",
      "    #                    bootstrap feature. You can use bootstrap-file more\r\n",
      "    #                    than once.\r\n",
      "    #bootstrap_python_package: [numpy,scipy,pandas]\r\n",
      "    #                    Path to a Python module to install on EMR. These\r\n",
      "    #                    should be standard python module tarballs where you\r\n",
      "    #                    can cd into a subdirectory and run ``sudo python\r\n",
      "    #                    setup.py install``. You can use bootstrap-python-\r\n",
      "    #                    package more than once.\r\n",
      "    #disable_emr_debugging:\r\n",
      "    #                    Disable storage of Hadoop logs in SimpleDB\r\n",
      "    ec2_core_instance_bid_price: '0.10'\r\n",
      "    #                    Bid price to specify for core (or \"slave\") nodes when\r\n",
      "    #                    setting them up as EC2 spot instances (you probably\r\n",
      "    #                    only want to set a bid price for task instances).\r\n",
      "    ec2_core_instance_type: c3.xlarge\r\n",
      "    #ec2_slave_instance_type: EC2_CORE_INSTANCE_TYPE\r\n",
      "    #                    Type of EC2 instance for core (or \"slave\") nodes only\r\n",
      "    #ec2_instance_type: EC2_INSTANCE_TYPE\r\n",
      "    #                    Type of EC2 instance(s) to launch (e.g. m1.small,\r\n",
      "    #                    c1.xlarge, m2.xlarge). See http://aws.amazon.com/ec2\r\n",
      "    #                    /instance-types/ for the full list.\r\n",
      "    ec2_key_pair: HadoopKeyPair\r\n",
      "    ec2_key_pair_file: /Users/yoavfreund/.ssh/HadoopKeyPair.pem\r\n",
      "    #                    Name of the SSH key pair you set up for EMR\r\n",
      "    #ec2_master_instance_bid_price: '0.30'\r\n",
      "    #                    Bid price to specify for the master node when setting\r\n",
      "    #                    it up as an EC2 spot instance (you probably only want\r\n",
      "    #                    to set a bid price for task instances).\r\n",
      "    ec2_master_instance_type: m3.xlarge\r\n",
      "    #                    Type of EC2 instance for master node only\r\n",
      "    #ec2_task_instance_bid_price: EC2_TASK_INSTANCE_BID_PRICE\r\n",
      "    #                    Bid price to specify for task nodes when setting them\r\n",
      "    #                    up as EC2 spot instances.\r\n",
      "    #ec2_task_instance_type: EC2_TASK_INSTANCE_TYPE\r\n",
      "    #                    Type of EC2 instance for task nodes only\r\n",
      "    num_ec2_core_instances: 5\r\n",
      "    #                    Number of EC2 instances to start as core (or \"slave\")\r\n",
      "    #                    nodes. Incompatible with num-ec2-instances.\r\n",
      "    #num_ec2_instances: NUM_EC2_INSTANCES\r\n",
      "    #                    Total number of EC2 instances to launch\r\n",
      "    #num_ec2_task_instances: NUM_EC2_TASK_INSTANCES\r\n",
      "    #                    Number of EC2 instances to start as task nodes.\r\n",
      "    #                    Incompatible with num-ec2-instances.\r\n",
      "    #emr_endpoint: EMR_ENDPOINT\r\n",
      "    #                    Optional host to connect to when communicating with S3\r\n",
      "    #                    (e.g. us-west-1.elasticmapreduce.amazonaws.com).\r\n",
      "    #                    Default is to infer this from aws_region.\r\n",
      "    #enable_emr_debugging:\r\n",
      "    #                    Enable storage of Hadoop logs in SimpleDB\r\n",
      "    #hadoop_version: HADOOP_VERSION\r\n",
      "    #                    Version of Hadoop to specify to EMR or to emulate for\r\n",
      "    #                    -r local. Default is 0.20.\r\n",
      "    max_hours_idle: 24\r\n",
      "    #                    If we create a persistent job flow, have it\r\n",
      "    #                    automatically terminate itself after it's been idle\r\n",
      "    #                    this many hours.\r\n",
      "    #mins_to_end_of_hour: MINS_TO_END_OF_HOUR\r\n",
      "    #                    If max-hours-idle is set, control how close to the\r\n",
      "    #                    end of an EC2 billing hour the job flow can\r\n",
      "    #                    automatically terminate itself (default is 5 minutes).\r\n",
      "    #no_pool_emr_job_flows\r\n",
      "    #                    Don't try to run our job on a pooled job flow.\r\n",
      "    #pool_emr_job_flows:\r\n",
      "    #                    Add to an existing job flow or create a new one that\r\n",
      "    #                    does not terminate when the job completes. Overrides\r\n",
      "    #                    other job flow-related options including EC2 instance\r\n",
      "    #                    configuration. Joins pool \"default\" if\r\n",
      "    #                    emr_job_flow_pool_name is not specified. WARNING: do\r\n",
      "    #                    not run this without\r\n",
      "    #                    mrjob.tools.emr.terminate_idle_job_flows in your\r\n",
      "    #                    crontab; job flows left idle can quickly become\r\n",
      "    #                    expensive!\r\n",
      "    # pool_name: YoavsPool\r\n",
      "    #                    Specify a pool name to join. Set to \"default\" if not\r\n",
      "    #                    specified.\r\n",
      "    #s3_endpoint: S3_ENDPOINT\r\n",
      "    #                    Host to connect to when communicating with S3 (e.g. s3\r\n",
      "    #                    -us-west-1.amazonaws.com). Default is to infer this\r\n",
      "    #                    from region (see aws-region).\r\n",
      "    s3_log_uri: s3://yoav.hadoop/logs/\r\n",
      "    #                    URI on S3 to write logs into\r\n",
      "    s3_scratch_uri: s3://yoav.hadoop/scratch/\r\n",
      "    #                    URI on S3 to use as our temp directory.\r\n",
      "    #s3_sync_wait_time: S3_SYNC_WAIT_TIME\r\n",
      "    #                    How long to wait for S3 to reach eventual consistency.\r\n",
      "    #                    This is typically less than a second (zero in us-west)\r\n",
      "    #                    but the default is 5.0 to be safe.\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/ubuntu/.mrjob.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
